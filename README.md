# KnowledgeDistillTransferPaperCollection

## 2014
### Distilling the Knowledge in a Neural Network NIPSW 2014 [[arXiv paper]](https://arxiv.org/abs/1503.02531)
"dark knowledge"

## 2015
### (FitNets) Fitnets Hints for Thin Deep Nets ICLR 2015 [[arXiv paper]](https://arxiv.org/abs/1412.6550)

## 2016
### Deep Model Compression Distilling Knowledge from Noisy Teachers arXiv 2016 [[paper]](https://arxiv.org/abs/1610.09650)

## 2017
### (Gram Matrix) A Gift from Knowledge Distillation Fast Optimization, Network Minimization and Transfer Learning CVPR 2017 [[paper]](http://openaccess.thecvf.com/content_cvpr_2017/html/Yim_A_Gift_From_CVPR_2017_paper.html)
### (Attention Map) Paying More Attention to Attention Improving the Performance of Convolutional Neural Networks via Attention Transfer ICLR 2017 [[paper]]()

## 2018
### Born Again Neural Networks ICML 2018 [[arXiv paper]](https://arxiv.org/abs/1805.04770)
### Knowledge Transfer with Jacobian Matching ICML 2018 [[paper]]()

## 2019
### Distilling Knowledge from a Deep Pose Regressor Network ICCV 2019 [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/html/Saputra_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network_ICCV_2019_paper.html)
### Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons AAAI 2019 [[paper]]()
### Learning Metrics From Teachers Compact Networks for Image Embedding CVPR 2019 [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Learning_Metrics_From_Teachers_Compact_Networks_for_Image_Embedding_CVPR_2019_paper.html) [[arXiv paper]](https://arxiv.org/abs/1904.03624) [[PyTorch code]](https://github.com/yulu0724/EmbeddingDistillation)
### Snapshot Distillation Teacher-Student Optimization in One Generation CVPR 2019 [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Snapshot_Distillation_Teacher-Student_Optimization_in_One_Generation_CVPR_2019_paper.html)

## 2020
### Distilling Cross-Task Knowledge via Relationship Matching CVPR 2020 [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Ye_Distilling_Cross-Task_Knowledge_via_Relationship_Matching_CVPR_2020_paper.html) [[PyTorch code]](https://github.com/njulus/ReFilled)
### Improved Knowledge Distillation via Teacher Assistant AAAI 2020 [[arXiv paper]](https://arxiv.org/abs/1902.03393)
### Inter-region Affinity Distillation for Road Marking Segmentation CVPR 2020 [[paper]]()
