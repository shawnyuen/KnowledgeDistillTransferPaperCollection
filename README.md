# KnowledgeDistillTransferPaperCollection

## Survey
### Knowledge Distillation A Survey IJCV 2021 [[IJCV paper]](https://link.springer.com/article/10.1007/s11263-021-01453-z) [[arXiv paper]](https://arxiv.org/abs/2006.05525)
### Knowledge Distillation and Student-Teacher Learning for Visual Intelligence A Review and New Outlooks IEEE TPAMI [[pami paper]](https://ieeexplore.ieee.org/document/9340578) [[arXiv paper]](https://arxiv.org/abs/2004.05937)

## 2014
### Distilling the Knowledge in a Neural Network NIPSW 2014 [[arXiv paper]](https://arxiv.org/abs/1503.02531)
"dark knowledge"

## 2015
### (FitNets) Fitnets Hints for Thin Deep Nets ICLR 2015 [[arXiv paper]](https://arxiv.org/abs/1412.6550)

## 2016
### Deep Model Compression Distilling Knowledge from Noisy Teachers arXiv 2016 [[paper]](https://arxiv.org/abs/1610.09650)

## 2017
### (Gram Matrix) A Gift from Knowledge Distillation Fast Optimization, Network Minimization and Transfer Learning CVPR 2017 [[paper]](http://openaccess.thecvf.com/content_cvpr_2017/html/Yim_A_Gift_From_CVPR_2017_paper.html)
### Like What You Like Knowledge Distill via Neuron Selectivity Transfer arXiv 2017 [[paper]](https://arxiv.org/abs/1707.01219)
### (Attention Map) Paying More Attention to Attention Improving the Performance of Convolutional Neural Networks via Attention Transfer ICLR 2017 [[paper]](https://openreview.net/forum?id=Sks9_ajex) [[code]](https://github.com/szagoruyko/attention-transfer)

## 2018
### An Embarrassingly Simple Approach for Knowledge Distillation arXiv 2018 [[paper]](https://arxiv.org/abs/1812.01819)
### Born Again Neural Networks ICML 2018 [[arXiv paper]](https://arxiv.org/abs/1805.04770)
### Knowledge Transfer with Jacobian Matching ICML 2018 [[arXiv paper]](https://arxiv.org/abs/1803.00443)
### Learning Deep Representations with Probabilistic Knowledge Transfer ECCV 2018 [[paper]](https://arxiv.org/abs/1803.10837) [[code]](https://github.com/passalis/probabilistic_kt)
### Learning Student Networks via Feature Embedding arXiv 2018 [[arXiv paper]](https://arxiv.org/abs/1812.06597)
### Paraphrasing Complex Network Network Compression via Factor Transfer NIPS 2018 [[paper]](https://papers.nips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer) [[arXiv paper]](https://arxiv.org/abs/1802.04977)
### Self-supervised Knowledge Distillation Using Singular Value Decomposition ECCV 2018 [[paper]](https://link.springer.com/chapter/10.1007/978-3-030-01231-1_21)

## 2019
### A Closer Look at Deep Learning Heuristics Learning rate restarts, Warmup and Distillation ICLR 2019 [[arXiv paper]](https://arxiv.org/abs/1810.13243)
### A Comprehensive Overhaul of Feature Distillation ICCV 2019 [[paper]](https://arxiv.org/abs/1904.01866)
### Correlation Congruence for Knowledge Distillation ICCV 2019 [[arXiv paper]](https://arxiv.org/abs/1904.01802)
### Distilling Knowledge from a Deep Pose Regressor Network ICCV 2019 [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/html/Saputra_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network_ICCV_2019_paper.html)
### Knowledge Distillation via Instance Relationship Graph CVPR 2019 [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Knowledge_Distillation_via_Instance_Relationship_Graph_CVPR_2019_paper.html)
### Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons AAAI 2019 [[paper]](https://www.aaai.org/ojs/index.php/AAAI/article/view/4264) [[arXiv paper]](https://arxiv.org/abs/1811.03233) [[code]](https://github.com/bhheo/AB_distillation)
### Learning Metrics From Teachers Compact Networks for Image Embedding CVPR 2019 [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Learning_Metrics_From_Teachers_Compact_Networks_for_Image_Embedding_CVPR_2019_paper.html) [[arXiv paper]](https://arxiv.org/abs/1904.03624) [[PyTorch code]](https://github.com/yulu0724/EmbeddingDistillation)
### LIT Learned Intermediate Representation Training for Model Compression ICML 2019 [[paper]](http://proceedings.mlr.press/v97/koratana19a.html) [[code]](https://github.com/stanford-futuredata/lit-code)
### MEAL Multi-Model Ensemble via Adversarial Learning AAAI 2019 [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/4417)
### On the Efficacy of Knowledge Distillation ICCV 2019 [[arXiv paper]](https://arxiv.org/abs/1910.01348)
### Relational Knowledge Distillation CVPR 2019 [[arXiv paper]](https://arxiv.org/abs/1904.05068) [[code]](https://github.com/lenscloth/RKD)
### Similarity-Preserving Knowledge Distillation ICCV 2019 [[arXiv paper]](https://arxiv.org/abs/1907.09682)
### Snapshot Distillation Teacher-Student Optimization in One Generation CVPR 2019 [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Snapshot_Distillation_Teacher-Student_Optimization_in_One_Generation_CVPR_2019_paper.html)
### Towards Understanding Knowledge Distillation ICML 2019 [[paper]](http://proceedings.mlr.press/v97/phuong19a.html)
### UM-Adapt Unsupervised Multi-Task Adaptation Using Adversarial Cross-Task Distillation ICCV 2019 [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/html/Kundu_UM-Adapt_Unsupervised_Multi-Task_Adaptation_Using_Adversarial_Cross-Task_Distillation_ICCV_2019_paper.html) [[arXiv paper]](https://arxiv.org/abs/1908.03884)

## 2020
### Channel-wise Distillation for Semantic Segmentation arXiv 2020 [[paper]](https://arxiv.org/abs/2011.13256) [[code]](https://github.com/drilistbox/CWD?v=1)
### Contrastive Representation Distillation ICLR 2020 [[paper]](https://openreview.net/forum?id=SkgpBJrtvS)
### Distilling Cross-Task Knowledge via Relationship Matching CVPR 2020 [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Ye_Distilling_Cross-Task_Knowledge_via_Relationship_Matching_CVPR_2020_paper.html) [[PyTorch code]](https://github.com/njulus/ReFilled)
### Improved Knowledge Distillation via Teacher Assistant AAAI 2020 [[arXiv paper]](https://arxiv.org/abs/1902.03393)
### Inter-region Affinity Distillation for Road Marking Segmentation CVPR 2020 [[paper]](https://arxiv.org/abs/2004.05304) [[code]](https://github.com/cardwing/Codes-for-IntRA-KD)
### Understanding and Improving Knowledge Distillation arXiv 2020 [[paper]](https://arxiv.org/abs/2002.03532)

## 2022
### A Fast Knowledge Distillation Framework for Visual Recognition ECCV 2022 [[paper]](https://arxiv.org/abs/2112.01528) [[code]](https://github.com/szq0214/FKD)
### Knowledge Distillation A Good Teacher Is Patient and Consistent CVPR 2022 [[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Beyer_Knowledge_Distillation_A_Good_Teacher_Is_Patient_and_Consistent_CVPR_2022_paper.html)
