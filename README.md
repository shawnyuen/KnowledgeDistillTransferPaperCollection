# KnowledgeDistillTransferPaperCollection

## Survey
### Distilling Knowledge via Knowledge - Review CVPR 2021 [[paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Distilling_Knowledge_via_Knowledge_Review_CVPR_2021_paper.html) [[arXiv paper]](https://arxiv.org/abs/2104.09044)
### Knowledge Distillation A Survey IJCV 2021 [[IJCV paper]](https://link.springer.com/article/10.1007/s11263-021-01453-z) [[arXiv paper]](https://arxiv.org/abs/2006.05525)
### Knowledge Distillation and Student-Teacher Learning for Visual Intelligence - A Review and New Outlooks IEEE TPAMI 2022 [[pami paper]](https://ieeexplore.ieee.org/document/9340578) [[arXiv paper]](https://arxiv.org/abs/2004.05937)

## 2023
### A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image IEEE ISBI 2023
### TinyMIM An Empirical Study of Distilling MIM Pre-trained Models arXiv 2023 [[arXiv paper]](https://arxiv.org/abs/2301.01296) [[PyTorch code]](https://github.com/OliverRensu/TinyMIM)
### Generalization Matters - Loss Minima Flattening via Parameter Hybridization for Efficient Online Knowledge Distillation CVPR 2023 [[arXiv paper]](https://arxiv.org/abs/2303.14666)
"The supervision loss of HWM can estimate the landscapeâ€™s curvature of the whole region around students to measure the generalization explicitly"

## 2022
### A Fast Knowledge Distillation Framework for Visual Recognition ECCV 2022 [[paper]](https://arxiv.org/abs/2112.01528) [[code]](https://github.com/szq0214/FKD)
### Adaptive Perspective Distillation for Semantic Segmentation IEEE TPAMI 2022 [[paper]](https://ieeexplore.ieee.org/document/9736597) [[zhao paper]](https://hszhao.github.io/papers/tpami22_apd.pdf)
"Adaptive Perspective Distillation, APD"
### Bag of Instances Aggregation Boosts Self-supervised Distillation ICLR 2022 [[paper]](https://openreview.net/forum?id=N0uJGWDw21d)
### CMKD CNN Transformer-Based Cross-Model Knowledge Distillation for Audio Classification arXiv 2022 [[paper]](https://arxiv.org/abs/2203.06760)
### Co-Advise Cross Inductive Bias Distillation CVPR 2022 [[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Ren_Co-Advise_Cross_Inductive_Bias_Distillation_CVPR_2022_paper.html) [[code]](https://github.com/OliverRensu/co-advise)
### Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation arXiv 2022 [[arXiv paper]](https://arxiv.org/abs/2205.14141)
### Cross-Image Relational Knowledge Distillation for Semantic Segmentation CVPR 2022 [[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Cross-Image_Relational_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2022_paper.html) [[code]](https://github.com/winycg/CIRKD)
"CIRKD"
### CXR Segmentation by AdaIN-Based Domain Adaptation and Knowledge Distillation ECCV 2022
### Decoupled Knowledge Distillation CVPR 2022 [[paper]](https://github.com/megvii-research/mdistiller)
### Distilling Inter-Class Distance for Semantic Segmentation IJCAI 2022 [[paper]](https://arxiv.org/abs/2205.03650)
""
### DistPro Searching A Fast Knowledge Distillation Process via Meta Optimization arXiv [[paper]](https://arxiv.org/abs/2204.05547)
### Efficient One Pass Self-distillation with Zipf's Label Smoothing ECCV 2022 [[paper]](https://arxiv.org/abs/2207.12980)
### Efficient Semantic Segmentation via Self-Attention and Self-Distillation IEEE TITS 2022 [[paper]](https://ieeexplore.ieee.org/document/9678134)
""
### FAKD - Feature Augmented Knowledge Distillation for Semantic Segmentation arXiv 2022 [[paper]](https://arxiv.org/abs/2208.14143) [[code]](https://github.com/jianlong-yuan/FAKD)
### GID - Global Information Distillation for Medical Semantic Segmentation Neurocomputing 2022 [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0925231222007950)
""
### Knowledge Distillation A Good Teacher Is Patient and Consistent CVPR 2022 [[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Beyer_Knowledge_Distillation_A_Good_Teacher_Is_Patient_and_Consistent_CVPR_2022_paper.html)
### Knowledge Distillation from A Stronger Teacher NeurIPS 2022
""
### Knowledge Distillation via the Target-Aware Transformer CVPR 2022
""
### Knowledge Distillation with Ensembles of Convolutional Neural Networks for Medical Image Segmentation 2022 JMI [[paper]](https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-9/issue-5/052407/Knowledge-distillation-with-ensembles-of-convolutional-neural-networks-for-medical/10.1117/1.JMI.9.5.052407.full)
""
### Knowledge Distillation with the Reused Teacher Classifier CVPR 2022 [[paper]](https://arxiv.org/abs/2203.14001) [[code]](https://github.com/DefangChen/SimKD)
### Masked Generative Distillation ECCV 2022 [[code]](https://github.com/yzd-v/MGD)
"MGD"
### Meta Knowledge Distillation arXiv 2022 [[paper]](https://arxiv.org/abs/2202.07940)
### Normalized Feature Distillation for Semantic Segmentation arXiv 2022 [[paper]](https://arxiv.org/abs/2207.05256)
""
### On the Benefits of Knowledge Distillation for Adversarial Robustness arXiv 2022 [[paper]](https://arxiv.org/abs/2203.07159)
### PrUE: Distilling Knowledge from Sparse Teacher Networks arXiv 2022 [[paper]](https://arxiv.org/abs/2207.00586)
### Pseudo Knowledge Distillation: Towards Learning Optimal Instance-specific Label Smoothing Regularization ICLR 2022 [[paper]](https://openreview.net/forum?id=SvFQBlffMB)
### Revisiting Self-Distillation arXiv 2022 [[paper]](https://arxiv.org/abs/2206.08491)
### SKDCGN Source-free Knowledge Distillation of Counterfactual Generative Networks using cGANs ECCVW 2022 [[paper]](https://arxiv.org/abs/2208.04226)
### Structural and Statistical Texture Knowledge Distillation for Semantic Segmentation CVPR 2022 [[arXiv paper]](https://arxiv.org/abs/2305.03944) [[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Ji_Structural_and_Statistical_Texture_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2022_paper.html)
""
### TransKD - Transformer Knowledge Distillation for Efficient Semantic Segmentation arXiv 2022 [[paper]](https://arxiv.org/abs/2202.13393)
"both teacher and student models are Transformers"
### What Knowledge Gets Distilled in Knowledge Distillation arXiv 2022 [[paper]](https://arxiv.org/abs/2205.16004)

## 2021
### A Boundary-aware Distillation Network for Compressed Video Semantic Segmentation ICPR 2021 [[paper]](https://ieeexplore.ieee.org/document/9412821)
"Pixel-wise Distillation and Inner-relation Distillation"
### Adaptive Distillation Aggregating Knowledge from Multiple Paths for Efficient Distillation BMVC 2021 [[arXiv paper]](https://arxiv.org/abs/2110.09674)
### AUTOKD Automatic Knowledge Distillation Into A Student Architecture Family arXiv 2021 [[arXiv paper]](https://arxiv.org/abs/2111.03555)
### Brain Tumor Segmentation based on Knowledge Distillation and Adversarial Training IJCNN 2021 [[paper]](https://ieeexplore.ieee.org/document/9534245)
"medical"
### Channel-Wise Knowledge Distillation for Dense Prediction ICCV 2021 [[paper]](https://openaccess.thecvf.com/content/ICCV2021/html/Shu_Channel-Wise_Knowledge_Distillation_for_Dense_Prediction_ICCV_2021_paper.html) [[arXiv paper]](https://arxiv.org/abs/2011.13256) [[code]](https://github.com/irfanICMLL/TorchDistiller/tree/main/SemSeg-distill)
""
### Does Knowledge Distillation Really Work NIPS 2021 [[paper]](https://proceedings.neurips.cc/paper/2021/hash/376c6b9ff3bedbbea56751a84fffc10c-Abstract.html) [[arXiv paper]](https://arxiv.org/abs/2106.05945)
### Double Similarity Distillation for Semantic Image Segmentation IEEE TIP 2021 [[paper]](https://ieeexplore.ieee.org/document/9444191)
""
### Efficient Knowledge Distillation for Liver CT Segmentation Using Growing Assistant Network PMB 2021 [[paper]](https://iopscience.iop.org/article/10.1088/1361-6560/ac3935/meta)
"medical"
### Efficient Medical Image Segmentation Based on Knowledge Distillation IEEE TMI 2021 [[paper]](https://ieeexplore.ieee.org/document/9491090) [[[arXiv paper]](https://arxiv.org/abs/2108.09987)
"real-time segmentation", "medical"
### Extrapolating from a Single Image to a Thousand Classes using Distillation arXiv 2021 [[paper]](https://arxiv.org/abs/2112.00725)
### Is Label Smoothing Truly Incompatible with Knowledge Distillation An Empirical Study ICLR 2021 [[paper]](https://openreview.net/forum?id=PObuuGVrGaZ) [[paper]](https://arxiv.org/abs/2104.00676)
### Real-time Semantic Segmentation via Sequential Knowledge Distillation Neurocomputing 2021 [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0925231221001685)
""
### Robust Semantic Segmentation With Multi-Teacher Knowledge Distillation IEEE Access 2021 [[paper]](https://ieeexplore.ieee.org/document/9522137)
### SEED Self-supervised Distillation For Visual Representation ICLR 2021 [[arXiv paper]](https://arxiv.org/abs/2101.04731)
### Semi-Supervising Learning Transfer Learning and Knowledge Distillation with SimCLR arXiv 2021 [[paper]](https://arxiv.org/abs/2108.00587)
### SimReg Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation BMVC 2021 [[paper]](https://arxiv.org/abs/2201.05131)
### Towards Efficient Medical Image Segmentation Via Boundary-Guided Knowledge Distillation IEEE ICME 2021 [[paper]](https://ieeexplore.ieee.org/document/9428395)
"medical"

## 2020
### Channel-wise Distillation for Semantic Segmentation arXiv 2020 [[paper]](https://arxiv.org/abs/2011.13256) [[code]](https://github.com/drilistbox/CWD?v=1)
### Contrastive Representation Distillation ICLR 2020 [[paper]](https://openreview.net/forum?id=SkgpBJrtvS)
### Distilling Cross-Task Knowledge via Relationship Matching CVPR 2020 [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Ye_Distilling_Cross-Task_Knowledge_via_Relationship_Matching_CVPR_2020_paper.html) [[PyTorch code]](https://github.com/njulus/ReFilled)
### Dreaming to Distill Data-Free Knowledge Transfer via DeepInversion CVPR 2020 [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.html) [[paper]](http://arxiv.org/abs/1912.08795)
### Feature-map-level Online Adversarial Knowledge Distillation ICML 2020 [[paper]](https://proceedings.mlr.press/v119/chung20a.html)
### Improved Knowledge Distillation via Teacher Assistant AAAI 2020 [[arXiv paper]](https://arxiv.org/abs/1902.03393)
### Inter-region Affinity Distillation for Road Marking Segmentation CVPR 2020 [[paper]](https://arxiv.org/abs/2004.05304) [[code]](https://github.com/cardwing/Codes-for-IntRA-KD)
### Intra-class Feature Variation Distillation for Semantic Segmentation ECCV 2020 [[paper]](https://link.springer.com/chapter/10.1007/978-3-030-58571-6_21) [[code]](https://github.com/YukangWang/IFVD)
"IFVD"
### Structured Knowledge Distillation for Dense Prediction IEEE TPAMI 2020 [[paper]](https://ieeexplore.ieee.org/document/9115859) [[code]](https://github.com/irfanICMLL/structure_knowledge_distillation)
### Understanding and Improving Knowledge Distillation arXiv 2020 [[paper]](https://arxiv.org/abs/2002.03532)

## 2019
### A Closer Look at Deep Learning Heuristics Learning Rate Restarts Warmup and Distillation ICLR 2019 [[arXiv paper]](https://arxiv.org/abs/1810.13243)
### A Comprehensive Overhaul of Feature Distillation ICCV 2019 [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/html/Heo_A_Comprehensive_Overhaul_of_Feature_Distillation_ICCV_2019_paper.html) [[arXiv paper]](https://arxiv.org/abs/1904.01866) [[PyTorch code]](https://github.com/clovaai/overhaul-distillation)
### Be Your Own Teacher Improve the Performance of Convolutional Neural Networks via Self Distillation ICCV 2019 [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Be_Your_Own_Teacher_Improve_the_Performance_of_Convolutional_Neural_ICCV_2019_paper.html)
### Correlation Congruence for Knowledge Distillation ICCV 2019 [[arXiv paper]](https://arxiv.org/abs/1904.01802)
### Distilling Knowledge from a Deep Pose Regressor Network ICCV 2019 [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/html/Saputra_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network_ICCV_2019_paper.html)
### Knowledge Adaptation for Efficient Semantic Segmentation CVPR 2019 [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/He_Knowledge_Adaptation_for_Efficient_Semantic_Segmentation_CVPR_2019_paper.html)
"real-time segmentation"
### Knowledge Distillation via Instance Relationship Graph CVPR 2019 [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Knowledge_Distillation_via_Instance_Relationship_Graph_CVPR_2019_paper.html)
### Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons AAAI 2019 [[paper]](https://www.aaai.org/ojs/index.php/AAAI/article/view/4264) [[arXiv paper]](https://arxiv.org/abs/1811.03233) [[code]](https://github.com/bhheo/AB_distillation)
### Learning Metrics From Teachers Compact Networks for Image Embedding CVPR 2019 [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Learning_Metrics_From_Teachers_Compact_Networks_for_Image_Embedding_CVPR_2019_paper.html) [[arXiv paper]](https://arxiv.org/abs/1904.03624) [[PyTorch code]](https://github.com/yulu0724/EmbeddingDistillation)
### LIT Learned Intermediate Representation Training for Model Compression ICML 2019 [[paper]](http://proceedings.mlr.press/v97/koratana19a.html) [[code]](https://github.com/stanford-futuredata/lit-code)
### MEAL Multi-Model Ensemble via Adversarial Learning AAAI 2019 [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/4417)
### On the Efficacy of Knowledge Distillation ICCV 2019 [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/html/Cho_On_the_Efficacy_of_Knowledge_Distillation_ICCV_2019_paper.html) [[arXiv paper]](https://arxiv.org/abs/1910.01348)
""
### Relational Knowledge Distillation CVPR 2019 [[arXiv paper]](https://arxiv.org/abs/1904.05068) [[code]](https://github.com/lenscloth/RKD)
### Similarity-Preserving Knowledge Distillation ICCV 2019 [[arXiv paper]](https://arxiv.org/abs/1907.09682)
### Snapshot Distillation Teacher-Student Optimization in One Generation CVPR 2019 [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Snapshot_Distillation_Teacher-Student_Optimization_in_One_Generation_CVPR_2019_paper.html)
### Structured Knowledge Distillation for Semantic Segmentation CVPR 2019 [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.html)
### Towards Understanding Knowledge Distillation ICML 2019 [[paper]](http://proceedings.mlr.press/v97/phuong19a.html)
### UM-Adapt Unsupervised Multi-Task Adaptation Using Adversarial Cross-Task Distillation ICCV 2019 [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/html/Kundu_UM-Adapt_Unsupervised_Multi-Task_Adaptation_Using_Adversarial_Cross-Task_Distillation_ICCV_2019_paper.html) [[arXiv paper]](https://arxiv.org/abs/1908.03884)
### When Does Label Smoothing Help NIPS 2019 [[paper]](https://proceedings.neurips.cc/paper/2019/hash/f1748d6b0fd9d439f71450117eba2725-Abstract.html)

## 2018
### An Embarrassingly Simple Approach for Knowledge Distillation arXiv 2018 [[arXiv paper]](https://arxiv.org/abs/1812.01819)
### Born Again Neural Networks ICML 2018 [[paper]](http://proceedings.mlr.press/v80/furlanello18a.html) [[arXiv paper]](https://arxiv.org/abs/1805.04770)
### Dataset Distillation arXiv 2018 [[arXiv paper]](https://arxiv.org/abs/1811.10959)
### Improving Fast Segmentation with Teacher-Student Learning BMVC 2018 [[arXiv paper]](https://arxiv.org/abs/1810.08476)
""
### KDGAN Knowledge Distillation with Generative Adversarial Networks NIPS 2018 [[paper]](https://proceedings.neurips.cc/paper/2018/hash/019d385eb67632a7e958e23f24bd07d7-Abstract.html)
### Knowledge Transfer with Jacobian Matching ICML 2018 [[arXiv paper]](https://arxiv.org/abs/1803.00443)
### Learning Deep Representations with Probabilistic Knowledge Transfer ECCV 2018 [[arXiv paper]](https://arxiv.org/abs/1803.10837) [[code]](https://github.com/passalis/probabilistic_kt)
### Learning Student Networks via Feature Embedding arXiv 2018 [[arXiv paper]](https://arxiv.org/abs/1812.06597)
### Paraphrasing Complex Network Network Compression via Factor Transfer NIPS 2018 [[paper]](https://proceedings.neurips.cc/paper/2018/hash/6d9cb7de5e8ac30bd5e8734bc96a35c1-Abstract.html) [[arXiv paper]](https://arxiv.org/abs/1802.04977) [[PyTorch code]](https://github.com/Jangho-Kim/Factor-Transfer-pytorch)
### Self-supervised Knowledge Distillation Using Singular Value Decomposition ECCV 2018 [[paper]](https://link.springer.com/chapter/10.1007/978-3-030-01231-1_21) [[cvf paper]](https://openaccess.thecvf.com/content_ECCV_2018/html/SEUNG_HYUN_LEE_Self-supervised_Knowledge_Distillation_ECCV_2018_paper.html) [[arXiv paper]](https://arxiv.org/abs/1807.06819) [[TensorFlow code]](https://github.com/sseung0703/SSKD_SVD)

## 2017
### (Gram Matrix) A Gift from Knowledge Distillation Fast Optimization Network Minimization and Transfer Learning CVPR 2017 [[paper]](http://openaccess.thecvf.com/content_cvpr_2017/html/Yim_A_Gift_From_CVPR_2017_paper.html)
### Like What You Like Knowledge Distill via Neuron Selectivity Transfer arXiv 2017 [[paper]](https://arxiv.org/abs/1707.01219)
### (Attention Map) Paying More Attention to Attention Improving the Performance of Convolutional Neural Networks via Attention Transfer ICLR 2017 [[paper]](https://openreview.net/forum?id=Sks9_ajex) [[code]](https://github.com/szagoruyko/attention-transfer)

## 2016
### Deep Model Compression Distilling Knowledge from Noisy Teachers arXiv 2016 [[paper]](https://arxiv.org/abs/1610.09650)

## 2015
### (FitNets) Fitnets Hints for Thin Deep Nets ICLR 2015 [[arXiv paper]](https://arxiv.org/abs/1412.6550)

## 2014
### Distilling the Knowledge in a Neural Network NIPSW 2014 [[arXiv paper]](https://arxiv.org/abs/1503.02531)
"dark knowledge"
