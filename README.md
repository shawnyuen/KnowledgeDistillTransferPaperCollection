# KnowledgeDistillTransferPaperCollection

## 2014
### (FitNets) Fitnets Hints for Thin Deep Nets arXiv 2014 [[paper]]()

## 2017
### (Gram Matrix) A Gift from Knowledge Distillation Fast Optimization, Network Minimization and Transfer Learning CVPR 2017 [[paper]](http://openaccess.thecvf.com/content_cvpr_2017/html/Yim_A_Gift_From_CVPR_2017_paper.html)
### (Attention Map) Paying More Attention to Attention Improving the Performance of Convolutional Neural Networks via Attention Transfer ICLR 2017 [[paper]]()

## 2018
### Knowledge Transfer with Jacobian Matching ICML 2018 [[paper]]()

## 2019
### Distilling Knowledge from a Deep Pose Regressor Network ICCV 2019 [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/html/Saputra_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network_ICCV_2019_paper.html)
### Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons AAAI 2019 [[paper]]()

## 2020
### Inter-region Affinity Distillation for Road Marking Segmentation CVPR 2020 [[paper]]()
